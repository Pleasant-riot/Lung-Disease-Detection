{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:528: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:529: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:530: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/opt/conda/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:535: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.keras.applications.vgg16 import VGG16\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/ops/resource_variable_ops.py:435: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = VGG16(input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "=================================================================\n",
      "Total params: 138,357,544\n",
      "Trainable params: 138,357,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.layers import Flatten, Dense, GlobalAveragePooling2D\n",
    "from tensorflow.python.keras.models import Model\n",
    "\n",
    "# x = GlobalAveragePooling2D()(model.output)\n",
    "# gap = GlobalAveragePooling2D(x)\n",
    "predictions = Dense(7, activation='softmax')(x) # output_shape \n",
    "custom_model = Model(model.input, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "_________________________________________________________________\n",
      "predictions (Dense)          (None, 1000)              4097000   \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 512)               512512    \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 7)                 3591      \n",
      "=================================================================\n",
      "Total params: 138,873,647\n",
      "Trainable params: 138,873,647\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "custom_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/ncp/workspace/data/dataset_.csv',index_col=0)\n",
    "\n",
    "dcm_list = list(data['orginal_dcm_file'])\n",
    "mask_path_list = list(data['body_part_file'])\n",
    "img_path_list = []\n",
    "\n",
    "for i in dcm_list:\n",
    "    png_img = i.replace('.dcm','.png')\n",
    "    img_path_list.append(png_img)\n",
    "\n",
    "def normal_dia(x):\n",
    "    if x == 9:\n",
    "        return 0\n",
    "    else:\n",
    "        return x\n",
    "    \n",
    "    \n",
    "data['diagnosis'] = data['diagnosis'].apply(normal_dia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "crop_list = list(data['crop_file'])\n",
    "\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "def cvt_crop(og_img_path, size):\n",
    "    \n",
    "    og_img = cv2.imread(og_img_path)\n",
    "    \n",
    "    \n",
    "    # clahe\n",
    "    clahe = cv2.createCLAHE(5.0,(8,8))\n",
    "    \n",
    "    ## og_img_convert\n",
    "    cvt_img = cv2.resize(og_img, size)\n",
    "    cvt_img = cv2.cvtColor(cvt_img,cv2.COLOR_BGR2GRAY)\n",
    "    cvt_img = clahe.apply(cvt_img)  # 1 channel\n",
    "    cvt_img = np.repeat(cvt_img[:,:,np.newaxis],3,-1)\n",
    "\n",
    "    return cvt_img\n",
    "\n",
    "\n",
    "\n",
    "def rv_3ch(og_img_path, mask_path, size):\n",
    "    \n",
    "    #module import\n",
    "    import cv2\n",
    "    \n",
    "    # im_read\n",
    "    og_img = cv2.imread(og_img_path)\n",
    "    mask = cv2.imread(mask_path)\n",
    "    \n",
    "    # clahe\n",
    "    clahe = cv2.createCLAHE(5.0,(12,12))\n",
    "    \n",
    "    ## og_img_convert\n",
    "    cvt_img = cv2.resize(og_img, size)\n",
    "    cvt_img = cv2.cvtColor(cvt_img,cv2.COLOR_BGR2GRAY)\n",
    "    cvt_img = clahe.apply(cvt_img)  # 1 channel\n",
    "    cvt_img = np.repeat(cvt_img[:,:,np.newaxis],3,-1)\n",
    "\n",
    "    ## mask\n",
    "    cvt_mask = cv2.resize(mask, size)\n",
    "    cvt_mask = cv2.cvtColor(cvt_mask, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    ## remove\n",
    "    remove = cv2.bitwise_and(cvt_img, cvt_img, mask = cvt_mask)\n",
    "    \n",
    "    return remove\n",
    "#     print(remove.shape)\n",
    "#     plt.imshow(remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.ndarray((4000,224,224,3))\n",
    "\n",
    "for i in range(len(X)):\n",
    "#     X[i][:][:][:] = rv_3ch(img_path_list[i], mask_path_list[i], (224,224))\n",
    "      X[i][:][:][:] = cvt_crop(crop_list[i], (224,224)).astype(np.uint8)\n",
    "    \n",
    "y = np.array(data['diagnosis'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "Xtr, Xva, Ytr, Yva = train_test_split(X,y, test_size=0.2, random_state=42, stratify=y)\n",
    "Xva, Xtst, Yva, Ytst = train_test_split(Xva,Yva, test_size=0.5, random_state=42, stratify=Yva)\n",
    "\n",
    "from tensorflow.python.keras.utils import to_categorical\n",
    "\n",
    "Ytr_c = to_categorical(Ytr)\n",
    "Yva_c = to_categorical(Yva)\n",
    "Ytst_c = to_categorical(Ytst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미지 증식\n",
    "\n",
    "from tensorflow.python.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "\n",
    "data_gen_args = dict()\n",
    "image_datagen = ImageDataGenerator(**data_gen_args)\n",
    "\n",
    "seed = 1\n",
    "\n",
    "train_generator = image_datagen.flow(x=Xtr, y=Ytr_c,\n",
    "                                    batch_size=8,\n",
    "                                    shuffle=True)\n",
    "validation_generator = image_datagen.flow(x=Xva, y=Yva_c,\n",
    "                                         batch_size=8,\n",
    "                                         shuffle=True)\n",
    "# test_generator = image_datagen.flow(x=Xtst, y=Ytst_c,\n",
    "#                                          batch_size=16,\n",
    "#                                          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras import backend as K\n",
    "import tensorflow.python as tf\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# #---------------클래스별 매트릭스 ---------------------------\n",
    "# def each_f1score(y_target, y_pred):\n",
    "#     y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "#     y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "#     target_names = [0,1,2,3,4,5,6]\n",
    "#     return classification_report(y_target_yn, y_pred_yn, target_names=target_names)\n",
    "# # print(classification_report(y_target_yn, y_pred_yn, target_names=target_names))\n",
    "# #---------------------------------------------------------\n",
    "\n",
    "def recall(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Negative) = 실제 값이 1(Positive) 전체\n",
    "    count_true_positive_false_negative = K.sum(y_target_yn)\n",
    "\n",
    "    # Recall =  (True Positive) / (True Positive + False Negative)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    recall = count_true_positive / (count_true_positive_false_negative + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_target, y_pred):\n",
    "    # clip(t, clip_value_min, clip_value_max) : clip_value_min~clip_value_max 이외 가장자리를 깎아 낸다\n",
    "    # round : 반올림한다\n",
    "    y_pred_yn = K.round(K.clip(y_pred, 0, 1)) # 예측값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "    y_target_yn = K.round(K.clip(y_target, 0, 1)) # 실제값을 0(Negative) 또는 1(Positive)로 설정한다\n",
    "\n",
    "    # True Positive는 실제 값과 예측 값이 모두 1(Positive)인 경우이다\n",
    "    count_true_positive = K.sum(y_target_yn * y_pred_yn) \n",
    "\n",
    "    # (True Positive + False Positive) = 예측 값이 1(Positive) 전체\n",
    "    count_true_positive_false_positive = K.sum(y_pred_yn)\n",
    "\n",
    "    # Precision = (True Positive) / (True Positive + False Positive)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    precision = count_true_positive / (count_true_positive_false_positive + K.epsilon())\n",
    "\n",
    "    # return a single tensor value\n",
    "    return precision\n",
    "\n",
    "def f1score(y_target, y_pred):\n",
    "    _recall = recall(y_target, y_pred)\n",
    "    _precision = precision(y_target, y_pred)\n",
    "    # K.epsilon()는 'divide by zero error' 예방차원에서 작은 수를 더한다\n",
    "    _f1score = ( 2 * _recall * _precision) / (_recall + _precision+ K.epsilon())\n",
    "    \n",
    "    # return a single tensor value\n",
    "    return _f1score\n",
    "\n",
    "class_weight = {0:0.571428563,\n",
    "                1:1.397990367,\n",
    "                2:4.617604545,\n",
    "                3:1.662337636,\n",
    "                4:0.892857129,\n",
    "                5:0.571428563,\n",
    "                6:1.181247674}\n",
    "\n",
    "\n",
    "from tensorflow.python.keras.optimizers import SGD\n",
    "\n",
    "\n",
    "# chexnet_model.compile( \n",
    "#     optimizer= 'nadam', \n",
    "#     loss='categorical_crossentropy', \n",
    "# #     metrics=[tf.keras.metrics.CategoricalAccuracy()])\n",
    "#     metrics=['accuracy',recall,precision,f1score])\n",
    "\n",
    "# # metrics=['accuracy',precision,f1score,each_f1score])\n",
    "\n",
    "\n",
    "# chexnet_model.compile( \n",
    "#   optimizer= 'adam', \n",
    "#   loss=f1_loss, \n",
    "#   metrics=['accuracy',f1])\n",
    "\n",
    " \n",
    "# class_weights = class_weight.compute_class_weight(\n",
    "#                 'balanced',\n",
    "#                  np.unique(train_generator.classes), \n",
    "#                  train_generator.classes)\n",
    "\n",
    "# chexnet_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy', precision, recall, f1score])\n",
    "# del X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy', precision, recall, f1score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "50/50 [==============================] - 3s 59ms/step - loss: 1.7933 - acc: 0.2500 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00\n",
      "400/400 [==============================] - 81s 202ms/step - loss: 1.7990 - acc: 0.2519 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 1.7933 - val_acc: 0.2500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 2/50\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 1.7975 - acc: 0.2500 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00\n",
      "400/400 [==============================] - 80s 199ms/step - loss: 1.7966 - acc: 0.2434 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 1.7975 - val_acc: 0.2500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 3/50\n",
      "50/50 [==============================] - 3s 54ms/step - loss: 1.7927 - acc: 0.2500 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00\n",
      "400/400 [==============================] - 80s 199ms/step - loss: 1.7976 - acc: 0.2441 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 1.7927 - val_acc: 0.2500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 4/50\n",
      "50/50 [==============================] - 3s 54ms/step - loss: 1.7932 - acc: 0.2500 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00\n",
      "400/400 [==============================] - 80s 199ms/step - loss: 1.7974 - acc: 0.2506 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 1.7932 - val_acc: 0.2500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 5/50\n",
      "50/50 [==============================] - 3s 55ms/step - loss: 1.7927 - acc: 0.2500 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00\n",
      "400/400 [==============================] - 80s 199ms/step - loss: 1.7975 - acc: 0.2497 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00 - val_loss: 1.7927 - val_acc: 0.2500 - val_precision: 0.0000e+00 - val_recall: 0.0000e+00 - val_f1score: 0.0000e+00\n",
      "Epoch 6/50\n",
      "303/400 [=====================>........] - ETA: 18s - loss: 1.7960 - acc: 0.2496 - precision: 0.0000e+00 - recall: 0.0000e+00 - f1score: 0.0000e+00"
     ]
    }
   ],
   "source": [
    "# chexnet_model.compile(optimizer='nadam', loss='binary_crossentropy',metrics=['acc'])\n",
    "import tensorflow as tf\n",
    "\n",
    "# history = chexnet_model.fit(Xtr,Ytr_c, validation_data=[Xva,Yva_c],batch_size=32, epochs=20)\n",
    "history2 = custom_model.fit_generator(train_generator, validation_data=validation_generator, \n",
    "                                       epochs=50, steps_per_epoch = 16,\n",
    "                                       verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
